# Serxy

`Serxy`是一款用Python编写的`代理服务器`与`代理池`的结合体，它可以根据客户端的HTTP/HTTPS请求中的`Host`字段来为该种请求创建并维护一个代理池。代理池的功能参考`Configuration/ProConfig.ini`配置文件。

python3.4+  

三方库 `requirement.txt`

数据库` mongodb`



### 更新日志

20180722 修改代理验证方法，尝试记录代理服务器的*Server*字段，为以后通过shodan抓取代理垫下基础。



### 说明

#### 添加爬虫方法

如果需要添加新的方法，定时到网站去爬取代理，操作如下：在ProxiesGetter/methods.py的class Methods 中添加一个方法，并声明`staticmethod ` ,方法名以 *method* 关键字开头，爬取的结果需要是 *addresss:port* 即 *192.168.1.1:8080* 这种格式的，然后将其放入 *GLOBAL.PRIORITY_QUEUE_2* 队列中即可。 

#### ServerConfig.ini

*DBProxiesGetterProcessInterval* 程序会定期对数据库中的代理进行可用性验证，这里设置该间隔的时间，单位秒

RowProxiesGetterProcessesInterval 程序会定期执行*Methods* 类中的以 *method* 开头的静态方法，该指令设置间隔时间

#### ProConfig.ini

配置文件有全局配置和指定配置，如何某个目标站点，没有设置特定的配置，默认使用全局配置

### 注意事项：

1）不要更改系统时间

2）mongodb 匿名访问



### 业务场景

1）爬虫使用，爬谷歌或是有一些IP反爬机制的网站

2）暴力破解，一些网站访问频率过高的就会对IP进行屏蔽



### 待解决或是无法解决的问题

#### 1）
本地代理服务器作为 客户端 与 远程代理服务器的 中间人，本地代理服务器与双方都建立了TCP套接字,在其 读取了客户端的请求数据后，会立即把数据发给远程代理服务器。在这期间，一旦获取到远程套接字的数据，就写回给客户端。但是，这期间很可能发生超时timeout的情况，也就是说客户端访问到的web界面是不完整的。

 这个情况在本地代理服务器这里，笔者认为是无法处理的，因为代理服务器无法知道一个HTTP Response的大小（HTTPS连接无法解决），所以针对这种超时后仅仅返回部分界面的情况，只能在客户端处理，客户端需要检查获取到的HTTP Response消息中的Content-Length 字段声明的长度是否与接收到的返回包的长度一致，如果不一致或是没有找到该字段，则考虑把包丢弃。

上面这种行为让我一开始写这个项目的一些想法无法实现，起初是想让sqlmap/burpsuit可以直接使用的，看来是不行了。sqlmap 假如需要使用该代理服务器，最好需要处理这种情况。当然，假如你的代理都是十分稳定的，不会发生建立套接字传输数据过程中超时中断的情况，上面这种情况可以不处理。

#### 2）
验证代理可用性的界面太简单了，数据只有几十个字节，但是我们通常访问的网页数据实际上达到了上万字节，所以表面上可用的代理，
实际下来可用性十分底，所以这里考虑自己搭建一个验证代理的界面。
